
**Attention Model Intuition**

![[Screenshot 2023-05-06 at 4.48.00 PM.png]]



# Seq2Seq Models with Attention


In a sequence-to-sequence (seq2seq) encoder-decoder model with attention, the goal is to generate a sequence of output tokens based on a sequence of input tokens. The model consists of two parts: an encoder that processes the input sequence and a decoder that generates the output sequence. The attention mechanism is used in the decoder to help the model focus on the most relevant parts of the input sequence while generating each output token.

The attention mechanism works as follows:

1. The encoder produces a sequence of hidden states $h_1, h_2, ..., h_n$ that represent the input sequence. 

2. At each step $t$ of the decoder, the decoder produces a hidden state $s_t$ that represents the decoder's current state. 

3. The attention mechanism calculates a set of attention weights $a^t_1, a^t_2, ..., a^t_n$ that indicate how much attention should be given to each hidden state $h_i$ of the encoder when generating the output at step $t$ of the decoder. 

	1. The attention weights are used to compute a context vector $c_t$ that is a weighted sum of the encoder hidden states, where each hidden state is weighted by its corresponding attention weight:

$$c_t = \sum_{i=1}^{n} a^t_i h_i$$

5. The context vector $c_t$ is concatenated with the decoder hidden state $s_t$ to form a combined vector $z_t$:

$$z_t = [c_t; s_t]$$

where $[;]$ denotes concatenation.

6. The combined vector $z_t$ is used to predict the probability distribution over the output vocabulary using a softmax function:

$$P_{vocab} = \text{softmax}(V'^T(Vz_t + b')$$

where $V'$ is a weight matrix and $b'$ is a bias term.

The attention weights $a^t_i$ are calculated using the following equations:

$$e_i^t= v^T \text{tanh}(W_h h_i + W_s s_t + b_{attn})$$

$$a^t_i = \frac{\text{exp}(e_i^t)}{\sum_{j=1}^{n} \text{exp}(e_j^t)}$$

where $v$, $W_h$, $W_s$, and $b_{attn}$ are learnable parameters.

The first equation computes a score $e_i^t$ for each encoder hidden state $h_i$ based on the decoder hidden state $s_t$. The second equation applies a softmax function to the scores to obtain the attention weights $a^t_i$ that sum to 1. The attention weights indicate the relative importance of each encoder hidden state $h_i$ at the current step $t$ of the decoder.

By using the attention mechanism, the model can selectively focus on different parts of the input sequence at each step of the decoder, improving the model's ability to generate accurate and relevant output tokens.

# Transformers


> **Calculating the parameters of transformers:**
* https://stackoverflow.com/a/71472362

	[Transformer Encoder-Decoder Architecture](https://i.stack.imgur.com/BhVnx.png) The BERT model contains only the encoder block of the transformer architecture. Let's look at individual elements of an encoder block for BERT to visualize the number weight matrices as well as the bias vectors. The given configuration L = 12 means there will be 12 layers of self attention, H = 768 means that the embedding dimension of individual tokens will be of 768 dimensions, A = 12 means there will be 12 attention heads in one layer of self attention. The encoder block performs the following sequence of operations:
	
	1.  The input will be the sequence of tokens as a matrix of S * d dimension. Where s is the sequence length and d is the embedding dimension. The resultant input sequence will be the sum of token embeddings, token type embeddings as well as position embedding as a d-dimensional vector for each token. In the BERT model, the first set of parameters is the vocabulary embeddings. BERT uses WordPiece[[2](https://arxiv.org/abs/1609.08144)] embeddings that has 30522 tokens. Each token is of 768 dimensions.
	    
	2.  Embedding layer normalization. One weight matrix and one bias vector.
	    
	3.  Multi-head self attention. There will be h number of heads, and for each head there will be three matrices which will correspond to query matrix, key matrix and the value matrix. The first dimension of these matrices will be the embedding dimension and the second dimension will be the embedding dimension divided by the number of attention heads. Apart from this, there will be one more matrix to transform the concatenated values generated by attention heads to the final token representation.
	    
	4.  Residual connection and layer normalization. One weight matrix and one bias vector.
	    
	5.  Position-wise feedforward network will have one hidden layer, that will correspond to two weight matrices and two bias vectors. In the paper, it is mentioned that the number of units in the hidden layer will be four times the embedding dimension.
	    
	6.  Residual connection and layer normalization. One weight matrix and one bias vector.
	    
	
	Let's calculate the actual number of parameters by associating the right dimensions to the weight matrices and bias vectors for the BERT base model.
	
	**Embedding Matrices:**
	
	-   Word Embedding Matrix size [Vocabulary size, embedding dimension] = [30522, 768] = 23440896
	-   Position embedding matrix size, [Maximum sequence length, embedding dimension] = [512, 768] = 393216
	-   Token Type Embedding matrix size [2, 768] = 1536
	-   Embedding Layer Normalization, weight and Bias [768] + [768] = 1536
	-   Total Embedding parameters = **𝟐𝟑𝟖𝟑𝟕𝟏𝟖𝟒 ≈ 𝟐𝟒𝑴**
	
	**Attention Head:**
	
	-   Query Weight Matrix size [768, 64] = 49152 and Bias [768] = 768
	    
	-   Key Weight Matrix size [768, 64] = 49152 and Bias [768] = 768
	    
	-   Value Weight Matrix size [768, 64] = 49152 and Bias [768] = 768
	    
	-   Total parameters for one layer attention with 12 heads = 12∗(3 ∗(49152+768)) = 1797120
	    
	-   Dense weight for projection after concatenation of heads [768, 768] = 589824 and Bias [768] = 768, (589824+768 = 590592)
	    
	-   Layer Normalization weight and Bias [768], [768] = 1536
	    
	-   Position wise feedforward network weight matrices and bias [3072, 768] = 2359296, [3072] = 3072 and [768, 3072 ] = 2359296, [768] = 768, (2359296+3072+ 2359296+768 = 4722432)
	    
	-   Layer Normalization weight and Bias [768], [768] = 1536
	    
	-   Total parameters for one complete attention layer (1797120 + 590592 + 1536 + 4722432 + 1536 = **7113216 ≈ 7𝑀**)
	    
	-   Total parameters for 12 layers of attention (𝟏𝟐 ∗ 𝟕𝟏𝟏𝟑𝟐𝟏𝟔 = **𝟖𝟓𝟑𝟓𝟖𝟓𝟗𝟐 ≈ 𝟖𝟓𝑴**)
	    
	
	**Output layer of BERT Encoder:**
	
	-   Dense Weight Matrix and Bias [768, 768] = 589824, [768] = 768, (589824 + 768 = 590592)
	
	_Total Parameters in 𝑩𝑬𝑹𝑻 𝑩ase = 𝟐𝟑𝟖𝟑𝟕𝟏𝟖𝟒 + 𝟖𝟓𝟑𝟓𝟖𝟓𝟗𝟐 + 𝟓𝟗𝟎𝟓𝟗𝟐 = **𝟏𝟎𝟗𝟕𝟖𝟔𝟑𝟔𝟖 ≈ 𝟏𝟏𝟎𝑴**_